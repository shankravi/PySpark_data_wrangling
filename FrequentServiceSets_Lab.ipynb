{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS/CMPSC 410 MiniProject #3\n",
    "\n",
    "### Spring 2020\n",
    "### Instructor: John Yen\n",
    "### TA: Rupesh Prajapati\n",
    "### Learning Objectives\n",
    "- Be able to identify top k services that are open for scanners in the Darknet dataset\n",
    "- Be able to identify frequent 2-service sets (based on top 10 services) and identify potentially interesting patterns\n",
    "- Be able to compute 3-service sets (based on top 10 services) that are open for scanners in the Darknet dataset\n",
    "\n",
    "### Total points: 30 \n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2: 10 points ( 5 points for part a, 5 points for part b)\n",
    "- Exercise 3: 15 points (10 points for part a, 5 points for part b)\n",
    "  \n",
    "### Due: 5 pm, April 17, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "from pyspark.sql.functions import col, column\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder.master(\"local\").appName(\"FrequentPortSet\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scanners_df = ss.read.csv(\"/storage/home/juy1/work/Darknet/scanners-dataset1-anon.csv\", header= True, inferSchema=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can use printSchema() to display the schema of the DataFrame Scanners_df to see whether it was inferred correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scanners_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A Transfosrm the feature \"host_services_per_censys\" into an array of services.\n",
    "### The original value of the column is a string that connects all the ports scanned by a scanner. The different services that are open by a scanner are connected by dash \"-\". For example, \"81-161-2000\" indicates the scanner has three ports/services open: port 81, port 161, and port 2000. Therefore, we want to use split to separate it into an array of ports/services open by each scanner.  This transformation is important because it enables the identification of frequent service sets open among scanners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The original value of the column \"host_services_per_censys\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scanners_df.select(\"host_services_per_censys\").show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to find the top 10 services/ports that most scanners have them open. There are multiple ways to find this information using Spark.  We will demonstrate one below, which involves the following steps: (The first five steps are identical to those for Mini Project Deliverable #2)\n",
    "1. Convert the DataFrame Scanners_df into an RDD\n",
    "2. Map to each row of the DF-converted RDD to extract the column \"host_services_per_censys\". Save the mapping result in a new RDD (which contains only values of the column).\n",
    "3. Use flatMap to split to the string (using \"-\" as the delimiter) to convert the RDD into an RDD of ports/services that are open on the host of the scanner.\n",
    "4. Use map to generate a key-value pair RDD, where key is a port/service opens on a scanner, the value is 1.\n",
    "5. Use reduceByKey to count the total number of scanners that have a specific port/service open.\n",
    "6. Sort the services, select the top 10 services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert the DataFrame Scanners_df into an RDD\n",
    "from pyspark.sql.functions import split\n",
    "Scanners_RDD = Scanners_df.rdd\n",
    "Scanners_RDD.persist().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Map to each row of the DF-converted RDD to extract the column \"host_services_per_censys\". Save the mapping result \n",
    "# in a new RDD (whih contains only values of the column)\n",
    "Host_services_column = Scanners_RDD.map(lambda row: row.host_services_per_censys)\n",
    "Host_services_column.persist().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can transform the string into a list of services (i.e., port numbers) that the scanner has opened using map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Host_services_rdd=Host_services_column.map(lambda string: string.split(\"-\"))\n",
    "Host_services_rdd.persist().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: However, in order to count how many scanners are keeping a specific port/service open, it is easier to use flatMap (instead of map above) to \"flatten\" the results of splitting (using \"-\" as the delimiter to convert the input RDD into an RDD of all ports/services that are open on the host of each sourceIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Host_services_f_list = Host_services_column.flatMap(lambda string: string.split(\"-\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use map to generate a key-value pair RDD, where key is a port/service opens on a scanner, the value is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Host_services_count = Host_services_f_list.map(lambda s: (s, 1) ) \n",
    "Host_services_count.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Use reduceByKey to count the total number of scanners that have a specific port/service open.\n",
    "\n",
    "1. Calculate the total number of scanners that have each port/service open. \n",
    "2. Sort them in descending order of count so that we can see the port/services that are open for most scanners. Save the resulted file in a directory you specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Host_services_total= Host_services_count.reduceByKey(lambda a, b: a+b, 1)\n",
    "Host_services_total.persist().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: Sort the port/service in descending order.\n",
    "Count_Services = Host_services_total.map(lambda x: tuple(reversed(x)) )\n",
    "Sorted_Count_Services = Count_Services.sortByKey(ascending=False)\n",
    "Sorted_Count_Services.persist().top(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sorted_Count_Services.saveAsTextFile(\"/storage/home/juy1/work/Darknet/Sorted_Service_Counts_by_Scanners10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sorted_Services=Sorted_Count_Services.map(lambda x: x[1]).collect()\n",
    "Sorted_Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: (5 points) Checking that the looping over the top 10 services to find frequent 2-service sets open by scanners is correct.  Since the order of the service in the 2-service set does not matter, you do not want to loop over a pair twice. For example, you do not want to include both \"80 and 443\" and \"443 and 80\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for Exercise 1\n",
    "for i in range(?,?):\n",
    "    for j in range(?,?):\n",
    "        print(Sorted_Services[i]+ \" and \" + Sorted_Services[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Par B Creating Service_Array column and 10 new columns to record whether each of top 10 services is open for each scanner.\n",
    "### B.1 Before we loop over the 2-service set, we need to first create the Service_Array column so that we can later check whether the array contains any of the specified services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scanners_df2=Scanners_df.withColumn(\"Services_Array\", split(col(\"host_services_per_censys\"), \"-\") )\n",
    "Scanners_df2.persist().show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.2 We create a new column to record whether each scanner has the top service (i.e., port 80) open using array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "Scanners_df2=Scanners_df2.withColumn(\"Top1_service\",array_contains(\"Services_Array\",\\\n",
    "                                                                       Sorted_Services[0]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to double check the total number of rows with \"Top1_service\" being true is the same as the number of scanners whose port 80 is open (calculated earlier to be 19362)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scanners_df_Top1=Scanners_df2.where(\"Top1_service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "Scanners_df_Top1.select(countDistinct(\"sourceIP\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.3 We are now ready to add other columns for other top services\n",
    "### The names of the new columns are Top2_service, Top3_service, ..., Top10_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    new_column_name=\"Top\"+str(i+1)+\"_service\"\n",
    "    Scanners_df2=Scanners_df2.withColumn(new_column_name,array_contains(\"Services_Array\",\\\n",
    "                                                                       Sorted_Services[i]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can check whether these new columns are actually created using show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scanners_df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C Determine the frequency of 2-service set (i.e., the number of scanners who have both services open for each 2-service set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "freq_table=np.empty([10,10])\n",
    "\n",
    "for i in range(0,10):\n",
    "    for j in range(i+1,10):\n",
    "        ### Calculate the frequency of 2-service set\n",
    "        print(\"Frequency of \"+Sorted_Services[i]+\" and \" + Sorted_Services[j] +\":\")\n",
    "        column_name_a=\"Top\"+str(i+1)+\"_service\"\n",
    "        column_name_b=\"Top\"+str(j+1)+\"_service\"\n",
    "        Scanners_df_temp= Scanners_df2.where(column_name_b).where(column_name_a)\n",
    "        freq_table[i,j]=Scanners_df_temp.count()\n",
    "        print(freq_table[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (10 points) (a) List 2-service set that has at least 1000 scanners.  List their frequency as well. (b) Identify at least one or two 2-service set that you believe is interesting.  Explain why you think it is interesting.\n",
    "## Answer to Exercise 2 (a): \n",
    "## Answer to Exercise 2 (b):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D Calculate the frequency of 3-service set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (15 points) (a) Calculate the frequency of 3-service set for top 10 services. (10 points) (b) List all three-service set that are open for at least 1000 scanners. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for Exercise 3 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer for Exercise 3 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
